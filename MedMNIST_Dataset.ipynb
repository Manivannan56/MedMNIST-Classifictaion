{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manivannan56/MedMNIST-Classifictaion/blob/main/MedMNIST_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9EJz9B0JQFx",
        "outputId": "e739feca-f5f9-4330-d3d8-f9802b0474b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting medmnist\n",
            "  Downloading medmnist-3.0.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.2.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.19.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from medmnist) (4.66.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from medmnist) (9.4.0)\n",
            "Collecting fire (from medmnist)\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from medmnist) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.16.0+cu121)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->medmnist) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->medmnist) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2023.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (3.2.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2024.2.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (23.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->medmnist) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->medmnist) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->medmnist) (1.3.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=bf9481c7a5e2cfc51b65c875300b0e623710d415d6abe1892664960876958f5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "Successfully built fire\n",
            "Installing collected packages: fire, medmnist\n",
            "Successfully installed fire-0.5.0 medmnist-3.0.1\n"
          ]
        }
      ],
      "source": [
        "pip install medmnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from  torch.utils.data import DataLoader,TensorDataset\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import medmnist\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n"
      ],
      "metadata": {
        "id": "QnA-RJnAhVw6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from medmnist import OrganAMNIST\n",
        "from medmnist import OrganCMNIST\n",
        "from medmnist import OrganSMNIST\n",
        "from medmnist import INFO"
      ],
      "metadata": {
        "id": "Yz66Qy8XKdCV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_1='organamnist'\n",
        "data_2='organcmnist'\n",
        "data_3='organsmnist'\n",
        "\n",
        "def get_details(data_flag):\n",
        "  info=INFO[data_flag]\n",
        "  task=info['task']\n",
        "  n_channels=info['n_channels']\n",
        "  n_classes=len(info['label'])\n",
        "\n",
        "\n",
        "\n",
        "  return info,task,n_channels,n_classes\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L9h1vrvqWwxJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info_1,task_1,n_channels_1,n_classes_1=get_details(data_1)\n",
        "info_2,task_2,n_channels_2,n_classes_2=get_details(data_2)\n",
        "info_3,task_3,n_channels_3,n_classes_3=get_details(data_3)\n",
        "\n",
        "Data_class1=getattr(medmnist,info_1['python_class'])\n",
        "Data_class2=getattr(medmnist,info_2['python_class'])\n",
        "Data_class3=getattr(medmnist,info_3['python_class'])\n",
        "\n"
      ],
      "metadata": {
        "id": "c6icFCThYfjt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_transformation=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5],std=[0.5])\n",
        "])\n",
        "\n",
        "download=True\n",
        "batch_size=64\n",
        "num_epochs=50"
      ],
      "metadata": {
        "id": "ztVSV3HWVHpW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from posixpath import split\n",
        "train_dataset_1=Data_class1(split='train',transform=Data_transformation,download=download)\n",
        "train_dataset_2=Data_class2(split='train',transform=Data_transformation,download=download)\n",
        "train_dataset_3=Data_class3(split='train',transform=Data_transformation,download=download)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hyNeDcOtLZ7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "199e4db8-111b-445b-b7fc-022d512a569d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://zenodo.org/records/10519652/files/organamnist.npz?download=1 to /root/.medmnist/organamnist.npz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 38247708/38247708 [00:02<00:00, 15414690.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://zenodo.org/records/10519652/files/organcmnist.npz?download=1 to /root/.medmnist/organcmnist.npz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15526411/15526411 [00:01<00:00, 10918531.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://zenodo.org/records/10519652/files/organsmnist.npz?download=1 to /root/.medmnist/organsmnist.npz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16528359/16528359 [00:01<00:00, 11711627.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def relabel_dataset(labels):\n",
        "  for i in labels:\n",
        "    i+=11\n",
        "  return labels"
      ],
      "metadata": {
        "id": "oGSJD5-Ha_LQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_2=relabel_dataset(train_dataset_2.labels)\n",
        "labels_3=relabel_dataset(train_dataset_3.labels)"
      ],
      "metadata": {
        "id": "MyupRivsaeTU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_1=np.array(train_dataset_1.imgs)\n",
        "labels_1=np.array(train_dataset_1.labels)\n",
        "images_2=np.array(train_dataset_2.imgs)\n",
        "labels_2=np.array(labels_2)\n",
        "images_3=np.array(train_dataset_3.imgs)\n",
        "labels_3=np.array(labels_3)\n",
        "\n",
        "images=np.concatenate((images_1,images_2,images_3),axis=0)\n",
        "labels=np.concatenate((labels_1,labels_2,labels_3),axis=0)\n",
        "\n",
        "len(images),len(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GamAecz_QkX",
        "outputId": "b343601a-b5c1-40aa-b956-d012eaaca13d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(61468, 61468)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels[50000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BpppDnqbm8N",
        "outputId": "248b3593-9c91-4dfe-f8f3-85f6180c774c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([15], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images=torch.from_numpy(images)\n",
        "\n",
        "images=images.unsqueeze(1).float()\n",
        "labels=labels.squeeze()\n"
      ],
      "metadata": {
        "id": "M6NXTviDii49"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f89y0rG22OHb",
        "outputId": "e979726e-1fb9-4362-c6bb-22537bc20f33"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(61468,)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels[50000]"
      ],
      "metadata": {
        "id": "--iBDJubfRkU",
        "outputId": "14d690b4-f35b-4997-883d-de9b02668873",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X=torch.tensor(images,dtype=torch.float32)\n",
        "Y=torch.tensor(labels,dtype=torch.long)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkxW2yZLP4NE",
        "outputId": "39bf3c37-11b4-4295-cf62-04820618765f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-d07992d93ea8>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X=torch.tensor(images,dtype=torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_val,Y_train,Y_val=train_test_split(X,Y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "fo6347pRJUUh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data=TensorDataset(X_train,Y_train)\n",
        "validation_data=TensorDataset(X_val,Y_val)\n",
        "\n",
        "train_loader=DataLoader(dataset=training_data,batch_size=batch_size,shuffle=True)\n",
        "val_loader=DataLoader(dataset=validation_data,batch_size=batch_size,shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "xqQgY6yFPItu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "1lDfDypB_SCO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1dHD9FH7_dWd",
        "outputId": "075b5216-5d7c-472d-905b-a54291debfb8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Modified_Resnet(nn.Module):\n",
        "  def __init__(self,n_classes=33):\n",
        "    super(Modified_Resnet,self). __init__()\n",
        "\n",
        "    self.resnet=models.resnet18(pretrained=True)\n",
        "\n",
        "    self.resnet.conv1=nn.Conv2d(1,64,kernel_size=3,stride=1,padding=1)\n",
        "\n",
        "    self.resnet.maxpool=nn.Identity()\n",
        "\n",
        "    num_filters=self.resnet.fc.in_features\n",
        "    self.resnet.fc= nn.Linear(num_filters,n_classes)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.resnet(x)\n"
      ],
      "metadata": {
        "id": "xHOebnRfkHOZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Modified_Resnet(n_classes=33)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCf4os8TZUGU",
        "outputId": "df4e7dce-69be-44ad-d855-2b2622520bb5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 55.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws4qwI51_jOA",
        "outputId": "478c2026-43fd-4780-8fe4-cb3e021353cd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Modified_Resnet(\n",
              "  (resnet): ResNet(\n",
              "    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): Identity()\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Linear(in_features=512, out_features=33, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion= nn.CrossEntropyLoss()\n",
        "optimizer= optim.Adam(model.parameters(),lr=0.001)"
      ],
      "metadata": {
        "id": "6YmX4uDbayd8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  train_loss=0.0\n",
        "  all_labels=[]\n",
        "  all_predictions=[]\n",
        "\n",
        "  for inputs,labels in train_loader:\n",
        "\n",
        "\n",
        "     inputs,labels=inputs.to(device),labels.to(device)\n",
        "     labels = labels.long()\n",
        "\n",
        "     optimizer.zero_grad()\n",
        "     outputs=model(inputs)\n",
        "     loss=criterion(outputs,labels)\n",
        "     loss.backward()\n",
        "     optimizer.step()\n",
        "\n",
        "     train_loss+=loss.item()*inputs.size(0)\n",
        "     _,predicted=torch.max(outputs.data,1)\n",
        "     all_labels.extend(labels.cpu().numpy())\n",
        "     all_predictions.extend(predicted.cpu().numpy())\n",
        "     train_report=classification_report(all_labels,all_predictions,zero_division=0)\n",
        "  train_loss/=len(train_loader.dataset)\n",
        "\n",
        "  model.eval()\n",
        "  val_loss=0.0\n",
        "  all_labels_val = []\n",
        "  all_predictions_val = []\n",
        "  for inputs,labels in val_loader:\n",
        "\n",
        "    inputs,labels=inputs.to(device),labels.to(device)\n",
        "    labels = labels.long()\n",
        "\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs=model(inputs)\n",
        "    loss=criterion(outputs,labels)\n",
        "    val_loss+=loss.item()*inputs.size(0)\n",
        "    _,predicted=torch.max(outputs.data,1)\n",
        "    all_labels_val.extend(labels.cpu().numpy())\n",
        "    all_predictions_val.extend(predicted.cpu().numpy())\n",
        "    val_report=classification_report(all_labels_val,all_predictions_val,zero_division=0)\n",
        "\n",
        "  val_loss/=len(val_loader.dataset)\n",
        "\n",
        "  print(f'Epoch {epoch+1}/{num_epochs},'\n",
        "          f'Training Loss: {train_loss:.4f},'\n",
        "          f'Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7JUr_f0hOA4",
        "outputId": "de17a495-9c62-48a8-c5d0-2beb9202e00a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50,Training Loss: 0.2188,Validation Loss: 0.2195\n",
            "Epoch 2/50,Training Loss: 0.1366,Validation Loss: 0.1797\n",
            "Epoch 3/50,Training Loss: 0.0932,Validation Loss: 0.1792\n",
            "Epoch 4/50,Training Loss: 0.0838,Validation Loss: 0.1003\n",
            "Epoch 5/50,Training Loss: 0.0636,Validation Loss: 0.1124\n",
            "Epoch 6/50,Training Loss: 0.0523,Validation Loss: 0.1065\n",
            "Epoch 7/50,Training Loss: 0.0478,Validation Loss: 0.1607\n",
            "Epoch 8/50,Training Loss: 0.0440,Validation Loss: 0.0950\n",
            "Epoch 9/50,Training Loss: 0.0378,Validation Loss: 0.1219\n",
            "Epoch 10/50,Training Loss: 0.0354,Validation Loss: 0.0950\n",
            "Epoch 11/50,Training Loss: 0.0277,Validation Loss: 0.0845\n",
            "Epoch 12/50,Training Loss: 0.0319,Validation Loss: 0.0994\n",
            "Epoch 13/50,Training Loss: 0.0227,Validation Loss: 0.0950\n",
            "Epoch 14/50,Training Loss: 0.0232,Validation Loss: 0.1035\n",
            "Epoch 15/50,Training Loss: 0.0271,Validation Loss: 0.1003\n",
            "Epoch 16/50,Training Loss: 0.0177,Validation Loss: 0.0997\n",
            "Epoch 17/50,Training Loss: 0.0211,Validation Loss: 0.0874\n",
            "Epoch 18/50,Training Loss: 0.0153,Validation Loss: 0.0880\n",
            "Epoch 19/50,Training Loss: 0.0168,Validation Loss: 0.1048\n",
            "Epoch 20/50,Training Loss: 0.0155,Validation Loss: 0.0887\n",
            "Epoch 21/50,Training Loss: 0.0158,Validation Loss: 0.1014\n",
            "Epoch 22/50,Training Loss: 0.0165,Validation Loss: 0.1247\n",
            "Epoch 23/50,Training Loss: 0.0156,Validation Loss: 0.1045\n",
            "Epoch 24/50,Training Loss: 0.0115,Validation Loss: 0.1044\n",
            "Epoch 25/50,Training Loss: 0.0117,Validation Loss: 0.0915\n",
            "Epoch 26/50,Training Loss: 0.0098,Validation Loss: 0.1055\n",
            "Epoch 27/50,Training Loss: 0.0110,Validation Loss: 0.1045\n",
            "Epoch 28/50,Training Loss: 0.0103,Validation Loss: 0.1179\n",
            "Epoch 29/50,Training Loss: 0.0122,Validation Loss: 0.0952\n",
            "Epoch 30/50,Training Loss: 0.0169,Validation Loss: 0.0945\n",
            "Epoch 31/50,Training Loss: 0.0077,Validation Loss: 0.1006\n",
            "Epoch 32/50,Training Loss: 0.0039,Validation Loss: 0.1161\n",
            "Epoch 33/50,Training Loss: 0.0117,Validation Loss: 0.1109\n",
            "Epoch 34/50,Training Loss: 0.0110,Validation Loss: 0.0962\n",
            "Epoch 35/50,Training Loss: 0.0071,Validation Loss: 0.0987\n",
            "Epoch 36/50,Training Loss: 0.0072,Validation Loss: 0.1163\n",
            "Epoch 37/50,Training Loss: 0.0109,Validation Loss: 0.1036\n",
            "Epoch 38/50,Training Loss: 0.0058,Validation Loss: 0.0942\n",
            "Epoch 39/50,Training Loss: 0.0080,Validation Loss: 0.1037\n",
            "Epoch 40/50,Training Loss: 0.0103,Validation Loss: 0.1097\n",
            "Epoch 41/50,Training Loss: 0.0084,Validation Loss: 0.0880\n",
            "Epoch 42/50,Training Loss: 0.0069,Validation Loss: 0.1028\n",
            "Epoch 43/50,Training Loss: 0.0042,Validation Loss: 0.0946\n",
            "Epoch 44/50,Training Loss: 0.0094,Validation Loss: 0.1026\n",
            "Epoch 45/50,Training Loss: 0.0074,Validation Loss: 0.0961\n",
            "Epoch 46/50,Training Loss: 0.0024,Validation Loss: 0.1281\n",
            "Epoch 47/50,Training Loss: 0.0086,Validation Loss: 0.0947\n",
            "Epoch 48/50,Training Loss: 0.0083,Validation Loss: 0.1220\n",
            "Epoch 49/50,Training Loss: 0.0055,Validation Loss: 0.1013\n",
            "Epoch 50/50,Training Loss: 0.0051,Validation Loss: 0.1163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(val_report)"
      ],
      "metadata": {
        "id": "QBzgTh3u1tLI",
        "outputId": "91675211-c198-4fe1-d483-9d83ef52a45b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99       368\n",
            "           1       0.99      1.00      0.99       313\n",
            "           2       0.98      0.99      0.98       267\n",
            "           3       1.00      1.00      1.00       296\n",
            "           4       0.99      0.99      0.99       743\n",
            "           5       0.98      0.99      0.99       765\n",
            "           6       1.00      1.00      1.00      1244\n",
            "           7       1.00      1.00      1.00       761\n",
            "           8       1.00      1.00      1.00       794\n",
            "           9       0.96      0.99      0.98       603\n",
            "          10       0.98      1.00      0.99       717\n",
            "          11       0.99      0.98      0.99       454\n",
            "          12       0.72      0.83      0.77       240\n",
            "          13       0.83      0.69      0.75       232\n",
            "          14       0.99      1.00      0.99       255\n",
            "          15       0.94      0.93      0.94       451\n",
            "          16       0.94      0.93      0.94       451\n",
            "          17       1.00      1.00      1.00      1299\n",
            "          18       1.00      1.00      1.00       351\n",
            "          19       1.00      0.99      1.00       373\n",
            "          20       0.97      0.93      0.95       694\n",
            "          21       0.98      0.98      0.98       623\n",
            "\n",
            "    accuracy                           0.98     12294\n",
            "   macro avg       0.96      0.96      0.96     12294\n",
            "weighted avg       0.98      0.98      0.98     12294\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dq0_igX3NizF",
        "outputId": "5404f1cd-a1a7-4df0-fe13-e70708efee01"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model,'/content/drive/MyDrive/model_entire_1.pth')"
      ],
      "metadata": {
        "id": "VNM4fhLa2dlz"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_test=(torch.load('/content/drive/MyDrive/model_entire_1.pth'))\n",
        "model_test.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNKR5VBjfj6i",
        "outputId": "a99754e9-808b-4232-b3c7-e593259f1200"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Modified_Resnet(\n",
              "  (resnet): ResNet(\n",
              "    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): Identity()\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Linear(in_features=512, out_features=33, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "true_labels = []\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():  # No gradients needed\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels=labels.to(device)\n",
        "        outputs = model_test(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Collect true labels and predictions\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "print(classification_report(true_labels, predictions))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xanfo9SOxYxO",
        "outputId": "dbb26ef5-0470-4616-ed75-cde6bb2de57b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99       368\n",
            "           1       0.99      1.00      0.99       313\n",
            "           2       0.98      0.99      0.98       267\n",
            "           3       1.00      1.00      1.00       296\n",
            "           4       0.99      0.99      0.99       743\n",
            "           5       0.98      0.99      0.99       765\n",
            "           6       1.00      1.00      1.00      1244\n",
            "           7       1.00      1.00      1.00       761\n",
            "           8       1.00      1.00      1.00       794\n",
            "           9       0.96      0.99      0.98       603\n",
            "          10       0.98      1.00      0.99       717\n",
            "          11       0.99      0.98      0.99       454\n",
            "          12       0.72      0.83      0.77       240\n",
            "          13       0.83      0.69      0.75       232\n",
            "          14       0.99      1.00      0.99       255\n",
            "          15       0.94      0.93      0.94       451\n",
            "          16       0.94      0.93      0.94       451\n",
            "          17       1.00      1.00      1.00      1299\n",
            "          18       1.00      1.00      1.00       351\n",
            "          19       1.00      0.99      1.00       373\n",
            "          20       0.97      0.93      0.95       694\n",
            "          21       0.98      0.98      0.98       623\n",
            "\n",
            "    accuracy                           0.98     12294\n",
            "   macro avg       0.96      0.96      0.96     12294\n",
            "weighted avg       0.98      0.98      0.98     12294\n",
            "\n"
          ]
        }
      ]
    }
  ]
}